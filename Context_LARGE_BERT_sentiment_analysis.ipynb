{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Context_LARGE_BERT_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNECj4BbGhQuSc4GYGFUqvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zen030/CourseProject/blob/main/Context_LARGE_BERT_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-69l_vOokYo"
      },
      "source": [
        "# **This notebook is implemented and tested in Google Colab PRO environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_kqpU75gc7"
      },
      "source": [
        "# 1. Colab Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYgfhvWdlo61",
        "outputId": "91910fef-dc46-4144-ba64-d9a89e8a920c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (50.3.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCCQxDTOnwis"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If GPU is available.\n",
        "if torch.cuda.is_available():    \n",
        "    # PyTorch to use the GPU    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If GPU is not available. Use the CPU.\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wETMfBQ0lLSQ"
      },
      "source": [
        "# To manage dataset\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# train.json file location: https://drive.google.com/file/d/1d5lwaHPOUBAz7c-cNXXQeFn75ZV2HkUh/view?usp=sharing\n",
        "# test.jsonl file location: https://drive.google.com/file/d/1vA3uyqy1TZmahgZ0PeNRFx67LuYeAkoW/view?usp=sharing\n",
        "\n",
        "# The training dataset\n",
        "# Google Drive file name\n",
        "training_file = 'train.jsonl'\n",
        "# Google Drive unique file ID\n",
        "training_file_id = '1d5lwaHPOUBAz7c-cNXXQeFn75ZV2HkUh'\n",
        "\n",
        "\n",
        "# The evaluation/testing dataset\n",
        "# Google Drive file name\n",
        "evaluation_file = 'test.jsonl'\n",
        "# Google Drive unique file ID\n",
        "test_jsonl_file_id = \"1vA3uyqy1TZmahgZ0PeNRFx67LuYeAkoW\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtoBZZGnzeP"
      },
      "source": [
        "# The files are shared to public.\n",
        "# Login using Google Account to proceed.\n",
        "# Copy-past the code.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':training_file_id})\n",
        "downloaded.GetContentFile(training_file)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':test_jsonl_file_id})\n",
        "downloaded.GetContentFile(evaluation_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtezXbQs5rur"
      },
      "source": [
        "# 2. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTheprk_lrur"
      },
      "source": [
        "# import modules\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHQ3l6ohAvpU"
      },
      "source": [
        "# Read jsonl file into list (of json)\n",
        "with open(training_file) as f:\n",
        "    # creating array of json\n",
        "    lines = f.read().splitlines()\n",
        "print(f'Number of lines in file: {len(lines)}')\n",
        "\n",
        "# Normalize json into dataframe columns\n",
        "df = pd.json_normalize(pd.DataFrame(lines)[0].apply(json.loads))\n",
        "print(f'Number of records in Pandas DataFrame: {len(df)}')\n",
        "\n",
        "# Lowercase response text.\n",
        "# BERT Model can lowercase the text in the setting.\n",
        "# I choose to lowercase the text here to have a uniform text format.\n",
        "# In case I need to modify the text for data cleaning before training the model.\n",
        "# df.response = df.response.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnPiAbQlAkI2"
      },
      "source": [
        "# Add 'label_value' integer column.\n",
        "#    0 for SARCASM\n",
        "#    1 for NOT_SARCASM\n",
        "label_dict = {'SARCASM': 0, 'NOT_SARCASM': 1}\n",
        "df['label_value'] = df.label.replace(label_dict)\n",
        "\n",
        "# Print maximum character length of 'response'\n",
        "max_response_chars = df.response.str.len().max()\n",
        "print(f\"Maximum character length of 'response': {max_response_chars}\")\n",
        "\n",
        "# Adding 5 extra characters in case special token is needed by model\n",
        "max_length = 512\n",
        "\n",
        "# Preview response data\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMbTCZA-hgrf"
      },
      "source": [
        "# Combine response and context\n",
        "df['response'] = df['response'] + '[SEP]'\n",
        "for index in df.index: \n",
        "  contexts = ''\n",
        "  for context in reversed(df['context'][index]):\n",
        "    contexts = contexts + ' ' + context\n",
        "  df['response'][index] = df['response'][index] + contexts\n",
        "\n",
        "# Check first record\n",
        "df.response[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyTGtCFH0Jt"
      },
      "source": [
        "# Preparing BERT Token for the training dataset.\n",
        "# BERT Tokenizing using HuggingFace Transformers library\n",
        "# (https://github.com/huggingface/transformers)\n",
        "bert_model = 'bert-large-uncased'\n",
        "batch_size = 1\n",
        "epochs = 4\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\n",
        "\n",
        "encoded_data_training = tokenizer.batch_encode_plus(\n",
        "    df.response.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "input_ids_training = encoded_data_training['input_ids']\n",
        "attention_masks_training = encoded_data_training['attention_mask']\n",
        "labels_training = torch.tensor(df.label_value.values)\n",
        "\n",
        "\n",
        "dataset_training = TensorDataset(input_ids_training, attention_masks_training, labels_training)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "data_loader_train = DataLoader(dataset_training, sampler=RandomSampler(dataset_training), batch_size=batch_size)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset_training)*epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenFOamfpRv_"
      },
      "source": [
        "#######################################################\n",
        "# Here we will train the model using training dataset #\n",
        "#######################################################\n",
        "\n",
        "from tqdm.notebook import tqdm # https://github.com/tqdm/tqdm\n",
        "import random\n",
        "\n",
        "# The random seed used to initialise the weights.\n",
        "# and select the order of the training data.\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "\n",
        "# All of the above assume the code was run on a CPU.\n",
        "# It is possible that when using the GPU to train the models, \n",
        "# the backend may be configured to use a sophisticated stack of GPU libraries, \n",
        "# and that some of these may introduce their own source of randomness.\n",
        "# For example, there is some evidence that if we are using Nvidia cuDNN, \n",
        "# that this may introduce additional sources of randomness and prevent \n",
        "# the exact reproducibility of results.\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# loop over the full dataset for a number of epochs times.\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    \n",
        "    # To set the model into a training mode.\n",
        "    model.train()\n",
        "    \n",
        "    # Measure the total training loss for each epoch.\n",
        "    loss_train_total = 0\n",
        "    # Progressbar to show the progress of the current epoch.\n",
        "    progress_bar = tqdm(data_loader_train, desc='Epoch {:1d}'.format(epoch+1), leave=False, disable=False)\n",
        "    \n",
        "    # Process each batch in the current epoch.\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a backward pass. \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Unpack current training batch.\n",
        "        # batch contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }       \n",
        "\n",
        "        # This is the actual learning.\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        # Current training loss.\n",
        "        loss = outputs[0]\n",
        "        # Current total training loss.\n",
        "        loss_train_total = loss_train_total + loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        \n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "         \n",
        "    # Save the trained BERT model for the current epoch iteration    \n",
        "    torch.save(model.state_dict(), f'CONTEXT_BASE_BERT_epoch_{epoch+1}.model')\n",
        "\n",
        "    # Report the summary of epoch iteration\n",
        "    tqdm.write(f'\\nEpoch {epoch+1} is completed')\n",
        "\n",
        "tqdm.write(f'\\n#########################')\n",
        "tqdm.write(f'\\n# Training is completed #')\n",
        "tqdm.write(f'\\n#########################')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDPdGFOi54MM"
      },
      "source": [
        "# 3. Evaluation\n",
        "\n",
        "To evaluate the trained model.\n",
        "The evaluation of trained model used to post 'answer.txt' is available here:\n",
        "https://github.com/zen030/CourseProject/blob/main/Evaluation_NAIVE_BERT_sentiment_analysis.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6nF2RTZ_ld"
      },
      "source": [
        "# Read testing/evaluation jsonl file into list (of json)\n",
        "evaluation_data_file = evaluation_file\n",
        "with open(evaluation_data_file) as f:\n",
        "    # creating array of json\n",
        "    lines = f.read().splitlines()\n",
        "print(f'Number of lines in file: {len(lines)}')\n",
        "\n",
        "# Normalize json into dataframe columns\n",
        "df = pd.json_normalize(pd.DataFrame(lines)[0].apply(json.loads))\n",
        "print(f'Number of records in Pandas DataFrame: {len(df)}')\n",
        "\n",
        "# Lowercase response text\n",
        "# BERT Model can lowercase the text in the setting\n",
        "# I choose to lowercase the text here to have a uniform text format\n",
        "# In case I need to modfiy the text for a reason\n",
        "df.response = df.response.str.lower()\n",
        "\n",
        "# Print maximum character length of 'response'\n",
        "max_response_chars = df.response.str.len().max()\n",
        "print(f\"Maximum character length of 'response': {max_response_chars}\")\n",
        "\n",
        "# Adding 5 extra characters in case special token is needed by model\n",
        "max_length = 512\n",
        "\n",
        "# Preview the response data\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RnK8j_jj0nn"
      },
      "source": [
        "# Combine response and context\n",
        "df['response'] = df['response'] + '[SEP]'\n",
        "for index in df.index: \n",
        "  contexts = ''\n",
        "  for context in reversed(df['context'][index]):\n",
        "    contexts = contexts + ' ' + context\n",
        "  df['response'][index] = df['response'][index] + contexts\n",
        "\n",
        "# Check first record\n",
        "df.response[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgLOsIbvskSO"
      },
      "source": [
        "bert_model = 'bert-large-uncased'\n",
        "batch_size = 5\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\n",
        "\n",
        "print(tokenizer)\n",
        "\n",
        "encoded_data_evaluation = tokenizer.batch_encode_plus(\n",
        "    df.response.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "input_ids_evaluation = encoded_data_evaluation['input_ids']\n",
        "attention_masks_evaluation = encoded_data_evaluation['attention_mask']\n",
        "\n",
        "dataset_evaluation = TensorDataset(input_ids_evaluation, attention_masks_evaluation)\n",
        "\n",
        "dataloader_eval = DataLoader(dataset_evaluation, sampler=SequentialSampler(dataset_evaluation), batch_size=batch_size)\n",
        "\n",
        "print(input_ids_evaluation[0])\n",
        "print(input_ids_evaluation[1])\n",
        "\n",
        "\n",
        "dataset_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN2sxGouu2ff"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# If GPU is available.\n",
        "if torch.cuda.is_available():    \n",
        "    # PyTorch to use the GPU    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If GPU is not available. Use the CPU.\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# To set the model into a training mode\n",
        "label_dict = {'SARCASM': 0, 'NOT_SARCASM': 1}\n",
        "bert_model = 'bert-large-uncased'\n",
        "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('CONTEXT_BASE_BERT_epoch_4.model', map_location=torch.device(device)))\n",
        "\n",
        "model.eval()\n",
        "loss_val_total = 0\n",
        "predictions = []\n",
        "\n",
        "for batch in dataloader_eval:\n",
        "  batch = tuple(b.to(device) for b in batch)\n",
        "  inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # evaluate the validation dataset\n",
        "    output = model(**inputs)\n",
        "    logits = output[0]\n",
        "    print(batch[0])\n",
        "    print(batch[1])\n",
        "    print(logits)\n",
        "    # print(output)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.append(logits)\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "print(preds_flat)\n",
        "\n",
        "print('Evaluation is done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnv-UWESg_D2"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens([12105])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db9MRxaE4Lnu"
      },
      "source": [
        "# Generate 'answer.txt'\n",
        "i = 1\n",
        "for pred in enumerate(preds_flat):\n",
        "  if pred[1] == 0:\n",
        "    text = 'SARCASM'\n",
        "  else:\n",
        "    text = 'NOT_SARCASM'\n",
        "  print('twitter_{0},{1}'.format(i, text))\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovc-DK-j58aV"
      },
      "source": [
        "# 4. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLZqg5Nn0-45"
      },
      "source": [
        "Final model and evaluation result are available here: \n",
        "https://github.com/zen030/CourseProject/blob/main/Evaluation_NAIVE_BERT_sentiment_analysis.ipynb"
      ]
    }
  ]
}